import requests
from bs4 import BeautifulSoup
from datetime import datetime


class Rent591Spider:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        self.session = requests.Session()

    def fetch_rent_info(self, url):
        response = requests.get(url, headers=self.headers)
        if response.status_code != 200:
            return {'error': f'è«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}', 'url': url}

        soup = BeautifulSoup(response.text, 'html.parser')
        try:
            landlord_raw = soup.select_one(
                '#__nuxt > section:nth-child(3) > section.main-wrapper > section.aside > section.contact-card > section > div.contact-info > p > span.name'
            )
            landlord_raw = landlord_raw.text.strip() if landlord_raw else 'ç„¡æ³•å–å¾—'
            if ': ' in landlord_raw:
                identity, name = landlord_raw.split(': ', 1)
            else:
                identity, name = 'æœªçŸ¥', landlord_raw

            house_type = soup.select_one(
                '#__nuxt > section:nth-child(3) > section.main-wrapper > section.main-content > section.block.info-board > div.pattern > span:nth-child(7)'
            )
            house_type = house_type.text.strip() if house_type else 'ç„¡æ³•å–å¾—'

            layout = soup.select_one(
                '#__nuxt > section:nth-child(3) > section.main-wrapper > section.main-content > section.block.info-board > div.pattern > span:nth-child(1)'
            )
            layout = layout.text.strip() if layout else 'ç„¡æ³•å–å¾—'

            desc = soup.select_one(
                '#__nuxt > section:nth-child(3) > section.main-wrapper > section.main-content > section.block.house-condition > div.house-condition-content > div.article'
            )
            desc = desc.text.strip() if desc else 'ç„¡æ³•å–å¾—'

            phone = soup.select_one(
                '#__nuxt > section:nth-child(3) > section.main-wrapper > section.aside > section.contact-card > div > div > button > span:nth-child(2) > span'
            )
            phone = phone.text.strip() if phone else 'ç„¡æ³•å–å¾—'

            return {
                'èº«ä»½': identity,
                'å§“å': name,
                'å‹æ…‹': house_type,
                'å±‹å­é…ç½®': layout,
                'å±‹æ³èªªæ˜': desc,
                'æ‰‹æ©Ÿ': phone
            }

        except Exception as e:
            return {'error': f'è³‡æ–™è§£æéŒ¯èª¤ï¼š{e}', 'url': url}

    def scrape_with_session(self, max_pages=None):
        """
        æŠ“å–ç§Ÿå±‹è³‡æ–™

        Args:
            max_pages (int, optional): æœ€å¤§æŠ“å–é æ•¸ï¼ŒNone è¡¨ç¤ºæŠ“å–æ‰€æœ‰é é¢

        Returns:
            list: æŠ“å–åˆ°çš„è³‡æ–™åˆ—è¡¨
        """
        self.session.get('https://rent.591.com.tw/', headers=self.headers, timeout=15)

        results = []
        page = 1

        while True:
            # æª¢æŸ¥æ˜¯å¦é”åˆ°æœ€å¤§é æ•¸é™åˆ¶
            if max_pages is not None and page > max_pages:
                print(f"âœ… å·²é”åˆ°è¨­å®šçš„æœ€å¤§é æ•¸é™åˆ¶ ({max_pages} é )ï¼Œåœæ­¢æŠ“å–ã€‚")
                break

            print(f"ğŸ“„ æ­£åœ¨è™•ç†ç¬¬ {page} é ...")
            target_url = f'https://rent.591.com.tw/list?region=8&page={page}'
            response = self.session.get(target_url, headers=self.headers, timeout=15)
            soup = BeautifulSoup(response.text, 'lxml')

            links = soup.select('a.link.v-middle')
            if not links:
                print(f"âœ… æ²’æœ‰æ›´å¤šè³‡æ–™ï¼Œç¬¬ {page} é ç‚ºæœ€å¾Œä¸€é ã€‚")
                break

            print(f"ğŸ” æ‰¾åˆ° {len(links)} ç­†æˆ¿å±‹é€£çµ\n")

            for i, a_tag in enumerate(links):
                href = a_tag.get('href')
                if not href.startswith("http"):
                    href = "https:" + href
                url = href
                title = a_tag.text.strip()

                # æŠ“è©³ç´°é è³‡æ–™
                detail = self.fetch_rent_info(url)

                data = {
                    'æ¨™é¡Œ': title,
                    'url': url,
                    **detail,
                    'æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }

                print(f"ç¬¬ {len(results) + 1} ç­†: {title}")
                results.append(data)

            page += 1

        print(f"\nğŸ“Š ç¸½å…±æŠ“å–äº† {len(results)} ç­†è³‡æ–™")
        return results





